{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from textblob import TextBlob\n",
    "from scipy.sparse import hstack, vstack\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.preprocessing import MaxAbsScaler, QuantileTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Carica il tuo DataFrame da 'train_B_text.csv'\n",
    "dataset = pd.read_csv('train_B_text.csv')\n",
    "simbol='\"'\n",
    "dataset['Counter quotmarks'] = dataset['Title'].str.count(simbol)\n",
    "def sentiment_calculator(title):\n",
    "    analysis = TextBlob(title)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1 #positive\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0 #neutral\n",
    "    else:\n",
    "        return -1 #negative\n",
    "    \n",
    "dataset['Count words'] = dataset['Title'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#dataset = dataset.drop('Id', axis=1)\n",
    "\n",
    "    \n",
    "def mean_length(title, n_words):\n",
    "    words = title.split() \n",
    "    total_length = sum(len(word) for word in words)\n",
    "    mean_length = total_length / n_words\n",
    "    return mean_length\n",
    "\n",
    "dataset[\"Mean length\"] = dataset.apply(lambda row: mean_length(row[\"Title\"], row[\"Count words\"]), axis=1)\n",
    "\n",
    "\n",
    "punctuation_symbols = [',', '.', ';',\"'\",\":\",'%','-']\n",
    "for i, symbol in enumerate(punctuation_symbols):\n",
    "    table_name = f'symbol_{i}'  # Nome della tabella con indice i\n",
    "    dataset[table_name] = dataset['Title'].str.count(symbol)\n",
    "\n",
    "dataset['Starts_with_quotmarks'] = dataset['Title'].str.startswith('\"').astype(int)\n",
    "\n",
    "\n",
    "def count_number(title):\n",
    "    number = re.findall(r'\\d+', title)\n",
    "    return len(number)\n",
    "dataset[\"Numbers\"]=dataset[\"Title\"].apply(count_number)\n",
    "\n",
    "\n",
    "\n",
    "dataset['Sentiment'] = dataset['Title'].apply(sentiment_calculator)\n",
    "dataset['Capital words'] = dataset['Title'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
    "dataset['Difference']=dataset['Count words']-dataset['Capital words']\n",
    "\n",
    "dataset['Count lword'] = dataset['Title'].apply(lambda x: len([word for word in x.split() if len(word)>2]))\n",
    "\n",
    "dataset['Difference2']=dataset['Count lword']-dataset['Capital words']\n",
    "\n",
    "# Estrai le colonne 'Title' e 'Fake/Real' dal DataFrame\n",
    "titles = dataset['Title']\n",
    "labels = dataset['Fake/Real']\n",
    "\n",
    "# Converti le etichette in 0 e 1\n",
    "labels = labels.map({'real': 1, 'fake': 0})\n",
    "\n",
    "corpus = dataset['Title'].tolist()\n",
    "sentences = [word_tokenize(title) for title in corpus]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=50, window=5, min_count=1)\n",
    "\n",
    "features_word2vec = []\n",
    "for sentence in sentences:\n",
    "    sentence_features = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if sentence_features:\n",
    "        features_word2vec.append(np.mean(sentence_features, axis=0)) \n",
    "    else:\n",
    "        features_word2vec.append(np.zeros(50)) \n",
    "\n",
    "\n",
    "# Converti 'features_word2vec' in un array numpy\n",
    "word2vec_array = np.array(features_word2vec)\n",
    "\n",
    "document_index = 0  \n",
    "cosine_similarities = cosine_similarity([word2vec_array[document_index]], word2vec_array)\n",
    "distance_metric = cosine_similarities[0]\n",
    "\n",
    "dataset['Cos similarity'] = distance_metric\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(dataset['Title'])\n",
    "\n",
    "\n",
    "def has_acronym(title):\n",
    "    words = title.split()  \n",
    "    for word in words:\n",
    "        if word.isupper() and len(word) > 1:\n",
    "            return 1 \n",
    "    return 0  \n",
    "\n",
    "dataset['Has Acronym'] = dataset['Title'].apply(has_acronym)\n",
    "\n",
    "X_combined =hstack((tfidf_matrix,dataset['Id'].values.reshape(-1, 1),dataset['Counter quotmarks'].values.reshape(-1, 1),\n",
    "                    dataset[\"Mean length\"].values.reshape(-1,1),dataset['Capital words'].values.reshape(-1,1),dataset['Difference2'].values.reshape(-1,1)\n",
    "                    ,dataset['Starts_with_quotmarks'].values.reshape(-1,1),\n",
    "                    dataset['symbol_0'].values.reshape(-1, 1),dataset['symbol_1'].values.reshape(-1, 1),dataset['symbol_2'].values.reshape(-1, 1),  \n",
    "                     dataset['symbol_3'].values.reshape(-1, 1),dataset['symbol_4'].values.reshape(-1, 1),  \n",
    "                     dataset['symbol_5'].values.reshape(-1, 1),dataset['symbol_6'].values.reshape(-1, 1),dataset['Has Acronym'].values.reshape(-1,1)\n",
    "                    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "datatest=pd.read_csv('test_B_text.csv')\n",
    "simbol='\"'\n",
    "datatest['Counter quotmarks'] = datatest['Title'].str.count(simbol)\n",
    "    \n",
    "datatest['Count words'] = datatest['Title'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    \n",
    "\n",
    "datatest[\"Mean length\"] = datatest.apply(lambda row: mean_length(row[\"Title\"], row[\"Count words\"]), axis=1)\n",
    "\n",
    "\n",
    "punctuation_symbols = [',', '.', \"'\",\":\"]\n",
    "datatest['punctuation'] = datatest['Title'].apply(lambda x: sum(x.count(symbol) for symbol in punctuation_symbols))\n",
    "\n",
    "\n",
    "datatest[\"Numbers\"]=datatest[\"Title\"].apply(count_number)\n",
    "datatest['Count lword'] = datatest['Title'].apply(lambda x: len([word for word in x.split() if len(word)>2]))\n",
    "\n",
    "\n",
    "datatest['Sentiment'] = datatest['Title'].apply(sentiment_calculator)\n",
    "datatest['Capital words'] = datatest['Title'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
    "\n",
    "\n",
    "datatest['Difference2']=datatest['Count lword']-datatest['Capital words']\n",
    "\n",
    "\n",
    "punctuation_symbols = [',', '.', ';',\"'\",\":\",'%','-']\n",
    "for i, symbol in enumerate(punctuation_symbols):\n",
    "    table_name = f'symbol_{i}'  # Nome della tabella con indice i\n",
    "    datatest[table_name] = datatest['Title'].str.count(symbol)\n",
    "\n",
    "datatest['Starts_with_quotmarks'] = datatest['Title'].str.startswith('\"').astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus = datatest['Title'].tolist()\n",
    "\n",
    "\n",
    "sentences = [word_tokenize(title) for title in corpus]\n",
    "model = Word2Vec(sentences, vector_size=50, window=5, min_count=1)\n",
    "\n",
    "features_word2vec = []\n",
    "for sentence in sentences:\n",
    "    sentence_features = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if sentence_features:\n",
    "        features_word2vec.append(np.mean(sentence_features, axis=0)) \n",
    "    else:\n",
    "        features_word2vec.append(np.zeros(50)) \n",
    "\n",
    "\n",
    "# Converti 'features_word2vec' in un array numpy\n",
    "word2vec_array = np.array(features_word2vec)\n",
    "document_index = 0  \n",
    "cosine_similarities = cosine_similarity([word2vec_array[document_index]], word2vec_array)\n",
    "distance_metric = cosine_similarities[0]\n",
    "datatest['Cos similarity'] = distance_metric\n",
    "\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.transform(datatest['Title'])\n",
    "\n",
    "def has_acronym(title):\n",
    "    words = title.split()  \n",
    "    for word in words:\n",
    "        if word.isupper() and len(word) > 1:\n",
    "            return 1 \n",
    "    return 0  \n",
    "\n",
    "datatest['Has Acronym'] = datatest['Title'].apply(has_acronym)\n",
    "\n",
    "X_combined_test=hstack((tfidf_matrix,datatest['Id'].values.reshape(-1, 1),datatest['Counter quotmarks'].values.reshape(-1, 1),datatest['Mean length'].values.reshape(-1,1),\n",
    "                    datatest['Capital words'].values.reshape(-1,1),datatest['Difference2'].values.reshape(-1,1)\n",
    "                    ,datatest['Starts_with_quotmarks'].values.reshape(-1,1),datatest['symbol_0'].values.reshape(-1, 1),datatest['symbol_1'].values.reshape(-1, 1),\n",
    "                    datatest['symbol_2'].values.reshape(-1, 1), datatest['symbol_3'].values.reshape(-1, 1),datatest['symbol_4'].values.reshape(-1, 1),  \n",
    "                     datatest['symbol_5'].values.reshape(-1, 1),datatest['symbol_6'].values.reshape(-1, 1),datatest['Has Acronym'].values.reshape(-1, 1)\n",
    "                    ))\n",
    "\n",
    "\n",
    "#tpot = TPOTClassifier(generations=3, population_size=20, random_state=42, verbosity=2)\n",
    "\n",
    "#best_model.fit(csr_matrix(X_combined),labels)\n",
    "#best_model=MLPClassifier(hidden_layer_sizes=(50, 50), alpha=0.0001, learning_rate_init=0.001)\n",
    "best_model = RandomForestClassifier(bootstrap=True, criterion='entropy', max_features=0.95, min_samples_leaf=1, min_samples_split=6, n_estimators=100)\n",
    "\n",
    "\n",
    "best_model.fit(X_combined, labels)\n",
    "\n",
    "\n",
    "predictions = best_model.predict(X_combined_test)\n",
    "\n",
    "Y_pred_testB = pd.Series(np.where(predictions==1, 'real', 'fake'))\n",
    "Y_pred_testB.to_csv('Y_pred_testB.10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabriela= pd.read_csv('y_prediction_v20.csv').iloc[:256,-1]\n",
    "y_prediction_v39=pd.concat([gabriela,pd.read_csv('Y_pred_testB.10.csv').iloc[:,-1]]).reset_index(drop=True).reset_index()\n",
    "y_prediction_v39.columns = ['Id','Prediction']\n",
    "y_prediction_v39.to_csv('y_prediction_v39.csv', sep=',',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
